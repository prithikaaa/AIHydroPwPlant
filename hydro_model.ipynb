{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ctgan import CTGAN\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HydroPwr\\venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:393: RuntimeWarning: overflow encountered in expm1\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ctgan import CTGAN\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "original_data = pd.read_csv('hydro_pw_plant_data.csv')\n",
    "\n",
    "# Separate input features and target columns\n",
    "input_features = original_data.iloc[:, :8]  # Assuming first 8 columns are input features\n",
    "target_columns = original_data.iloc[:, 8:]  # Assuming last 3 columns are target columns\n",
    "# Apply logarithmic transformation to ensure non-negativity\n",
    "transformer = FunctionTransformer(np.log1p, np.expm1)\n",
    "transformed_target_columns = transformer.fit_transform(input_features,target_columns)\n",
    "# Initialize CTGAN synthesizer\n",
    "ctgan = CTGAN(epochs=10)\n",
    "\n",
    "# Fit the CTGAN model to your data\n",
    "ctgan.fit(original_data)\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000  # Number of synthetic samples to generate\n",
    "synthetic_samples = ctgan.sample(num_samples)\n",
    "# Reverse the logarithmic transformation to obtain original scale\n",
    "inverse_transformed_samples = transformer.inverse_transform(synthetic_samples)\n",
    "\n",
    "# Convert synthetic samples to DataFrame\n",
    "synthetic_data = pd.DataFrame(inverse_transformed_samples, columns=target_columns.columns)\n",
    "\n",
    "# Optionally, combine synthetic samples with original data\n",
    "combined_data = pd.concat([original_data, synthetic_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "train = combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9.37747131645317e+114\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Separate features and targets\n",
    "X = train.iloc[:, :-3]  # Features\n",
    "y = train.iloc[:, -3:]  # Targets\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute the 85th percentile of the target variable\n",
    "lower_bound = np.percentile(y_train, 5)\n",
    "upper_bound = np.percentile(y_train, 47)\n",
    "# Initialize the imputer\n",
    "imputer = SimpleImputer(strategy='mean')  # You can change the strategy as needed\n",
    "\n",
    "# Fit the imputer on the training data and transform both training and test data\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "# Clip target variable to remove outliers\n",
    "y_train_clipped = np.clip(y_train, a_min=lower_bound, a_max=upper_bound)\n",
    "y_test_clipped = np.clip(y_test, a_min=lower_bound, a_max=upper_bound)\n",
    "\n",
    "# Convert clipped arrays back to DataFrame\n",
    "y_train_clipped_df = pd.DataFrame(y_train_clipped, columns=y.columns)\n",
    "y_test_clipped_df = pd.DataFrame(y_test_clipped, columns=y.columns)\n",
    "\n",
    "# Remove rows with NaN values\n",
    "y_train_clipped_df = y_train_clipped_df.dropna()\n",
    "y_test_clipped_df = y_test_clipped_df.dropna()\n",
    "\n",
    "# Convert back to numpy arrays\n",
    "y_train_clipped_cleaned = y_train_clipped_df.values\n",
    "y_test_clipped_cleaned = y_test_clipped_df.values\n",
    "\n",
    "# Initialize the base estimator (Random Forest Regressor)\n",
    "base_estimator = RandomForestRegressor()\n",
    "\n",
    "# Initialize the MultiOutput Regressor with the base estimator\n",
    "multioutput_regressor = MultiOutputRegressor(base_estimator)\n",
    "\n",
    "# Fit the MultiOutput Regressor to the training data\n",
    "multioutput_regressor.fit(X_train_imputed, y_train_clipped_cleaned)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = multioutput_regressor.predict(X_test_imputed)\n",
    "\n",
    "# Evaluate the model (for example, using Mean Squared Error)\n",
    "mse = mean_squared_error(y_test_clipped_cleaned, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.3357074471532206\n",
      "Mean Absolute Error: 2.1696049335611503e+57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test_clipped_cleaned, y_pred)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Calculate mean absolute error\n",
    "mae = mean_absolute_error(y_test_clipped_cleaned, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values for the last 3 columns:\n",
      "[[3027.98432785   80.55397381  102.99248233]]\n"
     ]
    }
   ],
   "source": [
    "input_row = np.array([7.0, 400.0, 2.0009, 200.777, 600.022, 190.0, 8.2, 6.02])\n",
    "input_row_reshaped = input_row.reshape(1, -1)\n",
    "output_values = multioutput_regressor.predict(input_row_reshaped)\n",
    "print(\"Predicted values for the last 3 columns:\")\n",
    "print(output_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl','wb') as files:\n",
    "    pickle.dump(multioutput_regressor,files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
